\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{hyperref}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage[left=2.00cm, right=2.00cm, top=2.00cm, bottom=2.00cm]{geometry}

\title{AN2DL Reports Template}

\begin{document}
    
    \begin{figure}[H]
        \raggedright
        \includegraphics[scale=0.4]{polimi.png} \hfill \includegraphics[scale=0.3]{airlab.jpeg}
    \end{figure}
    
    \vspace{5mm}
    
    \begin{center}
        % Select between First and Second
        {\Large \textbf{AN2DL - First/Second Homework Report}}\\
        \vspace{2mm}
        % Change with your Team Name
        {\Large \textbf{Team Name}}\\
        \vspace{2mm}
        % Team Members Information
        {\large Bettiati Matteo,}
        {\large Bianchi Lorenzo,}
        {\large Ostidich Francesco,}\\
        \vspace{2mm}
        % Codabench Nicknames
        {betti,}
        {lolly,}
        {kello,}\\
        \vspace{2mm}
        % Matriculation Numbers
        {258730,}
        {Matricola2,}
        {Matricola3,}\\
        \vspace{5mm}
        \today
    \end{center}    
    \vspace{5mm}

\section{Abstract}
The challenge involves developing a neural network model for binary classification on a dataset of blood cell images. Our initial step was to inspect the dataset to understand the nature of the images we would be working with. During this process, we identified and removed several outlier images to ensure the quality and consistency of the data. We also notice that the number of images per classes was not equal so we manage to augment the classes to make the dataset balanced. Given our initial lack of experience with image classification tasks, we decided to approach the problem using transfer learning with a pre-trained model. To balance computational efficiency and accuracy, we selected MobileNetV2, a model known for its favorable trade-off between parameter count and accuracy, as the foundation for our experiments. Later, we selected other models that were better suited to our purpose, incorporating fine-tuning to further enhance our performance.

\section{Operations on dataset}
The dataset consists of images designed for the classification of different types of blood cells. Each image is labeled with one of eight classes, representing various blood cell types such as Basophil, Eosinophil, Erythroblast,  Immature granulocytes, Lymphocyte, Monocyte, Neutrophil and Platelet. Image size is 96x96, color space RGB 3 channels.
\subsection{Data cleaning}
TODO: LOLLI
\subsection{Data augmentation}
TODO: KEKKO-LOLLI
\subsection{Data split}
TODO: KEKKO-LOLLI

\section{First network models and experiments}
Each team member began by selecting a pre-trained model, assigning hyperparameters, and implementing the code for the data preprocessing phase. Following this initial setup, we proceeded to design the model architecture. While the specific structures varied slightly between attempts, the core design remained consistent: place the pre-trained model after the input, followed by a GAP layer, a batch normalization layer, two dense layer each one with a drop out layer and finally the output.
\subsection{Hyperparameters}
Hyperparameter have been changed a lot during the development phase, here we are just gonna put the most used. In the \hyperref[hyperparametertuning]{Hyperparameter Tuning} you can find all the values used and how their change affected our models.
Here instead the most used ones:\\
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Parameter} & \textbf{Transfer Learning} & \textbf{Fine Tuning} \\ \hline
Learning Rate & 1e-4 & 1e-5 \\ \hline
Weight Decay & 1e-3 & 1e-3 \\ \hline
Epochs & 10 & 5 \\ \hline
Batch Size & 32 & 64 \\ \hline
\end{tabular}
\caption{Hyperparameters for Transfer Learning and Fine Tuning}
\end{table}
\subsection{Preprocessing data}
All the pre-trained models we selected come with an already implemented data preprocessing phase. For our work, we utilized the class\_weight function to adjust the weights when the dataset was unbalanced. However, after performing data augmentation to balance the classes, we no longer needed to use this function.
\subsection{Model structure}
The model structure has also being change during the development phase.
Here you will find a scheme of the most used one:
\begin{table}[h!]
\centering
\begin{tabular}{|c|}
\hline
\textbf{Layer (type)} \\ \hline
Input (InputLayer)\\ \hline
model\_name (Functional)\\ \hline
GAP (GlobalAveradgePooling2D)\\ \hline
Hidden (Dense)\\ \hline
Dropout (Dropout)\\ \hline
Hidden (Dense)\\ \hline
Dropout (Dropout)\\ \hline
Output (Dense)\\ \hline
\end{tabular}

\caption{Most used architecture in the development phase.}
\end{table}
\subsection{Transfer learning}
From the beginning we had in mind to divide the training phase in transfer learn and, once reached a decent accuracy, start to fine tuned in order to allow the model to leverage the knowledge it gained. To avoid overfitting we implemented early stopping, dropout and regularization techniques.
\subsection{Fine tuning}
 After achieving decent performance through transfer learning, we began fine-tuning. This is the process of unfreezing more layers and allowing the model to adjust those layers' weights to better fit our data. Fine-tuning allows the model to make more specific adjustments based on the patterns in your dataset. At the beginning we unfroze all the layer at the same time, than we chose to follow the more correct procedure of unfreeze gradually the layers.
\section{Comparison between the models}
\subsection{Used models and comparison overview}
Important note: the first model used were tested and trained on the first augmented datasets. Later in the competition we have uploaded new augmented datasets, but we had already discard some models. This is the reason for eventually misleading values of accuracy.
\begin{table}[h!]
\centering

\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model Name} & \textbf{Transfer Learning} & \textbf{Fine tuning}& \textbf{Local Test set} \\
\hline
MobileNetV2         & 0.5102 & 0.9163 & 0.9151 \\ \hline
EfficientNetB4      & 0.7084 & 0.0.8436 & 0.8809 \\ \hline
EfficientNetV2S     & 0.6898 & 0.8844 & 0.9321 \\ \hline
EfficientNetV2M     &  &  & \\ \hline
ConvNeXtSmall       &  &  & \\ \hline
DenseNet201         &  &  & \\ \hline
Xception            &  &  & \\ \hline
ConvNeXtTiny        &  &  & \\ \hline
\end{tabular}
\caption{List of models used in the experiment with their accuracy in the various training phases.}
\end{table}

\subsection{Hyperparameter tuning}
\label{hyperparametertuning}
KELLO divertiti.


\section{Final model}
ConvNeXtTiny-LOLLI


\section{Contributions}

All the members of the team contributed equally to the project. Here is a list of the features that each member contributed the most to:
\begin{itemize}
    \item betti helped with the setup of the work environments, trained and tested new models and fine tuned all the parameter in order to make the models perform at best.
    \item lolly worked hard on data, cleaning and augmenting the dataset and providing the best datasets to work on.
    \item kello helped with the augmentation, training and testing process. He land us a hand taking part to all the work phases.
\end{itemize}




\section{References}
NON CI SERVONO REFERENCES, ABBIAMO LOLLI IN SQUADRA.
LOLLI SVELA DOVE HAI RECAPITATO I SEGRETI.

\end{document}
