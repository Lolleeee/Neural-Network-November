\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{hyperref}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage[left=2.00cm, right=2.00cm, top=2.00cm, bottom=2.00cm]{geometry}

\title{AN2DL Reports Template}

\begin{document}
    
    \begin{figure}[H]
        \raggedright
        \includegraphics[scale=0.4]{polimi.png} \hfill \includegraphics[scale=0.3]{airlab.jpeg}
    \end{figure}
    
    \vspace{5mm}
    
    \begin{center}
        % Select between First and Second
        {\Large \textbf{AN2DL - First/Second Homework Report}}\\
        \vspace{2mm}
        % Change with your Team Name
        {\Large \textbf{Team Name}}\\
        \vspace{2mm}
        % Team Members Information
        {\large Bettiati Matteo,}
        {\large Bianchi Lorenzo,}
        {\large Ostidich Francesco,}\\
        \vspace{2mm}
        % Codabench Nicknames
        {betti,}
        {lolly,}
        {kello,}\\
        \vspace{2mm}
        % Matriculation Numbers
        {258730,}
        {259946,}
        {Matricola3,}\\
        \vspace{5mm}
        \today
    \end{center}    
    \vspace{5mm}

\section{Abstract}
The challenge involves developing a neural network model for multiclass classification on a dataset of blood cell images. Our initial step was to inspect the dataset to understand the nature of the images we would be working with. During this process, we identified and removed several outlier images to ensure the quality and consistency of the data. We also notice that the number of images per classes was not equal so we manage to augment the classes to make the dataset balanced. Given our initial lack of experience with image classification tasks and the relatively small size of the dataset given, we decided to approach the problem using transfer learning with a pre-trained model. To balance computational efficiency and accuracy, we selected \textit{MobileNetV2}, a model known for its favorable trade-off between parameter count and accuracy, as the foundation for our experiments. Later, we selected other models that were better suited to our purpose, incorporating fine-tuning to further enhance our performance.

\section{Operations on dataset}
The raw dataset consists in total of 13759 images designed for the classification of different types of blood cells. Each image is labeled with one of eight classes, representing various blood cell types such as Basophil, Eosinophil, Erythroblast,  Immature granulocytes, Lymphocyte, Monocyte, Neutrophil and Platelet. Image size is 96x96, color space RGB 3 channels with \textit{uint8} data type.

\subsection{Exploratory Data Analysis}
The first step in a good machine learning pipeline is exploratory data analysis (EDA). To perform EDA, we plotted a grid sorted by class, which allowed us to identify two types of outliers. The first was a repeated image containing a watermark of Shrek that appeared across multiple classes. The second was a repeated image featuring a watermark of Rickroll in the Monocyte class. Both outliers were easily removed as they were identical images repeated without variation, making them straightforward to detect.

An analysis of class distribution revealed a mildly severe class imbalance. To address this issue, we employed image duplication and data augmentation to ensure more balanced representation across classes.

Additionally, we identified some ambiguously labeled images, such as IM1 and IM2. Since there were no clearly mislabeled samples and considering the overall dataset size, we opted to retain these ambiguous images. This decision was made to preserve inherent diversity and sparsity in the dataset, which can contribute to the robustness of the model.
\subsection{Data augmentation}
During the project data augmentation has been a paramount stage which influenced greatly the performance of the model. The progressive refinement of the pipeline lead to 3 main iterations of data augmentation pipeline.
All of which handled class unbalance and performed data splitting either by custom functions or using \textit{train\_test\_split} by \textit{scikit\-learn}

The first one was done by kello so I dont know what he'd done.

The second one same

The third approach, which was ultimately used in the final submission model, began by balancing the label distribution. This was achieved by duplicating the underrepresented classes just enough to match the number of samples in the largest class. The entire dataset was then split and fed into a \textit{TensorFlow Dataset}, which managed the batch size, shuffling, and mapping of the augmentation pipeline.

This approach made the augmentation process dynamic rather than static, as augmentations were applied in real-time during runtime. By handling augmentations dynamically, the model was exposed to a broader variety of augmented samples, enhancing generalization and reducing the risk of overfitting. However, this method was adopted only as a last resort, as it significantly slowed down the training process, even though it was a necessary step to achieve the final optimal performance.

\section{First network models and experiments}
Each team member began by selecting a pre-trained model, adjusting hyperparameters like learning rate and batch size, and getting a feel of the complexity of the task.
Starting with relatively small models like MobileNet to leverage fast training and less overfitting risk given the small dataset. 
Following this initial setup, we proceeded to design the top layers architecture. Overall, While the specific structures varied slightly between attempts, the core design remained consistent: place the pre-trained model after the input layer, followed by a \textit{GlobalAveragePooling2D} layer, followed by a normalization and dropout layer, after which 2 blocks of \textit{Dense-Normalization-Dropout} layers where used to get finally to the output layer of 8 neurons for the 8 labels.
CIAOOOOOO
\subsection{Hyperparameters}
Hyperparameter have been changed a lot during the development phase, here we are just gonna put the most used. In the \hyperref[hyperparametertuning]{Hyperparameter Tuning} you can find all the values used and how their change affected our models.
Here instead the most used ones:\\
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Phase} & \textbf{Top Layers Training} & \textbf{Fine Tuning} \\ \hline
Learning Rate & 1e-4 & 1e-5 \\ \hline
Weight Decay & 1e-3 & 1e-3 \\ \hline
Epochs & 10 & 20 \\ \hline
Batch Size & 32 & 64 \\ \hline
\end{tabular}
\caption{Most used hyperparameters in training}
\end{table}
\subsection{Preprocessing data (decidere dove metterlo)}
All the pre-trained models we selected come with an already implemented data preprocessing phase. For our work, we utilized the \textit{class\_weight} function to adjust the weights when the dataset was unbalanced. However, after performing data augmentation to balance the classes, we no longer needed to use this function.
\subsection{Model structure}
The model structure has also being change during the development phase.
Here you will find a scheme of the most used setup:
\begin{table}[h!]
\centering
\begin{tabular}{|c|}
\hline
\textbf{Layer (type)} \\ \hline
Input (InputLayer)\\ \hline
\textit{base\_model} (Functional)\\ \hline
GAP (GlobalAveradgePooling2D)\\ \hline
Hidden (Dense)\\ \hline
Batch\_Normalization (Batch\_Normalization)\\ \hline
Dropout (Dropout)\\ \hline
Hidden (Dense)\\ \hline
Batch\_Normalization (Batch\_Normalization)\\ \hline
Dropout (Dropout)\\ \hline
Output (Dense)\\ \hline
\end{tabular}

\caption{Most used architecture in the development phase.}
\end{table}
\subsection{Transfer learning}
From the outset, we planned to divide the transfer learning process into two phases: an initial training phase focused on the top layers, followed by fine-tuning the base model once accuracy plateaued. This strategy aimed to enable the model to better leverage the pre-trained weights. To mitigate overfitting, we implemented early stopping, learning rate schedulers, dropout, and regularization techniques.

All training was conducted using Colab and Kaggle GPUs. To address resource limitations, we utilized checkpoints and mixed precision training. Additionally, in some runs, TensorFlow datasets were employed to optimize RAM usage during training.
\subsection{Fine tuning}
After achieving a plateau in performance during the top layer training phase, we transitioned to fine-tuning. This process involves unfreezing layers of the base model, allowing the model to adjust their weights to better fit the specific patterns in our dataset. Fine-tuning enables more targeted adjustments, leveraging the general features learned during pretraining while adapting to the nuances of our data.

Initially, we unfroze all the layers simultaneously. However, we later adopted the more gradual and widely recommended approach of unfreezing layers incrementally. Gradual unfreezing helps to prevent the pretrained model from completely overwriting its learned representations, preserving useful features while allowing adaptation. Unfreezing all layers at once can lead to catastrophic forgetting, where the model loses valuable generalizable knowledge from pretraining. This gradual approach also reduces the risk of overfitting, as early layers (which often capture fundamental features like edges and textures) remain relatively stable during the initial stages of fine-tuning.
\section{Comparison between the models}
\subsection{Used models and comparison overview}
Important note: the first model used were tested and trained on the first augmented datasets. Later in the competition we have uploaded new augmented datasets, but we had already discard some models. This is the reason for eventually misleading values of accuracy.
\begin{table}[h!]
\centering

\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model Name} & \textbf{Transfer Learning} & \textbf{Fine tuning}& \textbf{Local Test set} \\
\hline
MobileNetV2         & 0.5102 & 0.9163 & 0.9151 \\ \hline
EfficientNetB4      & 0.7084 & 0.0.8436 & 0.8809 \\ \hline
EfficientNetV2S     & 0.6898 & 0.8844 & 0.9321 \\ \hline
EfficientNetV2M     &  &  & \\ \hline
ConvNeXtSmall       &  &  & \\ \hline
DenseNet201         &  &  & \\ \hline
Xception            &  &  & \\ \hline
ConvNeXtTiny        &  &  & \\ \hline
\end{tabular}
\caption{List of models used in the experiment with their accuracy in the various training phases.}
\end{table}

\subsection{Hyperparameter tuning}
\label{hyperparametertuning}
KELLO divertiti.


\section{Final model}
ConvNeXtTiny-LOLLI


\section{Contributions}

All the members of the team contributed equally to the project. Here is a list of the features that each member contributed the most to:
\begin{itemize}
    \item betti helped with the setup of the work environments, trained and tested new models and fine tuned all the parameter in order to make the models perform at best.
    \item lolly worked hard on data, cleaning and augmenting the dataset and providing the best datasets to work on.
    \item kello helped with the augmentation, training and testing process. He land us a hand taking part to all the work phases.
\end{itemize}




\section{References}
NON CI SERVONO REFERENCES, ABBIAMO LOLLI IN SQUADRA.
LOLLI SVELA DOVE HAI RECAPITATO I SEGRETI.

\end{document}
