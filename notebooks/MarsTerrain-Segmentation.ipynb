{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true,
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CO6_Ft_8T56A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import folding as fold\n",
    "import plotter as plot\n",
    "\n",
    "def when():\n",
    "    now = datetime.now(pytz.timezone(\"Europe/Rome\")).strftime(\"Day %Y-%m-%d - Time %H:%M:%S\\n\")\n",
    "    print(f\"\\033[1;94m{now}\\033[0m\")\n",
    "\n",
    "when()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when()\n",
    "\n",
    "# data = np.load('/kaggle/input/marsterrain-collage/dataset_collage.npz')\n",
    "data = np.load('/kaggle/input/marsterrain-pumped/dataset_pumped.npz')\n",
    "\n",
    "images = data['images'][..., np.newaxis]\n",
    "labels = data['labels']\n",
    "num_classes = len(np.unique(data['labels']))\n",
    "\n",
    "test = np.load('/kaggle/input/marsterrain-original/dataset_original.npz')\n",
    "test = test['test_set']\n",
    "test = test[..., np.newaxis]\n",
    "\n",
    "print(f\"Dataset: {images.dtype}{images.shape}, {labels.dtype}{labels.shape}\")\n",
    "print(f\"Classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when()\n",
    "\n",
    "model_name = \"Gorizia\"\n",
    "\n",
    "batch_size = 32\n",
    "patience = 50\n",
    "epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-3\n",
    "reduce_factor = 0.5\n",
    "\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "print(f\"patience: {patience}\")\n",
    "print(f\"epochs: {epochs}\")\n",
    "print(f\"learning_rate: {learning_rate}\")\n",
    "print(f\"weight_decay: {weight_decay}\")\n",
    "print(f\"reduce_factor: {reduce_factor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when() \n",
    "\n",
    "input_shape = images[0].shape\n",
    "\n",
    "def conv_block(input_tensor, filters, kernel_size=3, activation='relu', dropout_rate=0.1, name=''):\n",
    "    x = input_tensor\n",
    "    x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', kernel_initializer='he_normal', name=name+'conv1')(x)\n",
    "    x = tfkl.BatchNormalization(name=name+'bn1')(x)\n",
    "    x = tfkl.Activation(activation, name=name+'act1')(x)\n",
    "    \n",
    "    x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', kernel_initializer='he_normal', name=name+'conv2')(x)\n",
    "    x = tfkl.BatchNormalization(name=name+'bn2')(x)\n",
    "    x = tfkl.Activation(activation, name=name+'act2')(x)\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        x = tfkl.Dropout(dropout_rate, name=name+'dropout')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def attention_gate(x, g, inter_channels):\n",
    "    theta_x = tfkl.Conv2D(inter_channels, kernel_size=1, strides=1, padding='same')(x)\n",
    "    phi_g = tfkl.Conv2D(inter_channels, kernel_size=1, strides=1, padding='same')(g)\n",
    "    \n",
    "    concat = tfkl.Add()([theta_x, phi_g])\n",
    "    relu_concat = tfkl.Activation('relu')(concat)\n",
    "    \n",
    "    psi = tfkl.Conv2D(1, kernel_size=1, padding='same', activation='sigmoid')(relu_concat)\n",
    "    \n",
    "    return tfkl.Multiply()([x, psi])\n",
    "\n",
    "# Input layer\n",
    "input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "# Enhanced Downsampling Path\n",
    "down_block_1 = conv_block(input_layer, 32, name='down_block1_')\n",
    "d1 = tfkl.MaxPooling2D()(down_block_1)\n",
    "down_block_2 = conv_block(d1, 64, name='down_block2_')\n",
    "d2 = tfkl.MaxPooling2D()(down_block_2)\n",
    "\n",
    "# Bottleneck with Dilated Convolutions\n",
    "bottleneck = tfkl.Conv2D(128, 3, padding='same', dilation_rate=2, activation='relu')(d2)\n",
    "bottleneck = tfkl.BatchNormalization()(bottleneck)\n",
    "bottleneck = tfkl.Conv2D(128, 3, padding='same', dilation_rate=2, activation='relu')(bottleneck)\n",
    "\n",
    "# Upsampling Path with Attention Gates\n",
    "u1 = tfkl.UpSampling2D(interpolation='bilinear')(bottleneck)\n",
    "skip_connection_1 = attention_gate(down_block_2, u1, 64)\n",
    "u1 = tfkl.Concatenate()([u1, skip_connection_1])\n",
    "u1 = conv_block(u1, 64, name='up_block1_')\n",
    "\n",
    "u2 = tfkl.UpSampling2D(interpolation='bilinear')(u1)\n",
    "skip_connection_2 = attention_gate(down_block_1, u2, 32)\n",
    "u2 = tfkl.Concatenate()([u2, skip_connection_2])\n",
    "u2 = conv_block(u2, 32, name='up_block2_')\n",
    "\n",
    "# Output Layer with additional refinement\n",
    "output_layer = tfkl.Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\", name='output_layer')(u2)\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='Enhanced_UNet')\n",
    "\n",
    "# Show model summary\n",
    "# model.summary(expand_nested=True, show_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when()\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels.flatten())\n",
    "print([f\"{w:.4f}\" for w in class_weights])\n",
    "class_weights[0] *= 0.01\n",
    "print([f\"{w:.4f}\" for w in class_weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when()\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=reduce_factor,     \n",
    "    patience=10,        \n",
    "    min_lr=1e-6,     \n",
    "    verbose=0   \n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='mean_io_u', \n",
    "    patience=patience,       \n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1    \n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f'{model_name}.keras', \n",
    "    monitor='mean_io_u',          \n",
    "    save_best_only=True,       \n",
    "    save_weights_only=False,     \n",
    "    mode='max',\n",
    "    verbose=0                 \n",
    ")\n",
    "\n",
    "class QuietCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, print_every=10):\n",
    "        super().__init__()\n",
    "        self.print_every = print_every\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        \n",
    "        epochs = self.params['epochs']\n",
    "        progress = f\"\\033[1;91mEpoch {epoch+1}/{epochs}{' ' * (len(str(epochs)) - len(str(epoch+1)))}\\033[0m   \" \\\n",
    "                   f\"Accuracy {logs.get('val_accuracy', 0):.4f} - \" \\\n",
    "                   f\"Loss {logs.get('val_loss', 0):.4f} - \" \\\n",
    "                   f\"Mean IoU {logs.get('val_mean_io_u', 0):.4f} - \" \\\n",
    "                   f\"Learning Rate {logs.get('learning_rate', 0):.4e}\"\n",
    "        \n",
    "        if (epoch+1) % self.print_every == 0:\n",
    "            print(progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when()\n",
    "\n",
    "def focal_loss(class_weights=None, gamma=2.0, alpha=0.25):\n",
    "    def static_focal_loss(y_true, y_pred, class_weights=class_weights, gamma=gamma, alpha=alpha):\n",
    "        # Get number of classes from prediction shape\n",
    "        num_classes = tf.shape(y_pred)[-1]\n",
    "        \n",
    "        # Prepare class weights\n",
    "        if class_weights is None:\n",
    "            class_weights = tf.ones(num_classes, dtype=tf.float32)\n",
    "        else:\n",
    "            class_weights = tf.convert_to_tensor(class_weights, dtype=tf.float32)\n",
    "        \n",
    "        # One-hot encode true labels if not already\n",
    "        y_true_oh = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes)\n",
    "        \n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Compute cross entropy\n",
    "        cross_entropy = -y_true_oh * tf.math.log(y_pred)\n",
    "        \n",
    "        # Compute focal weights\n",
    "        probs = tf.reduce_sum(y_true_oh * y_pred, axis=-1, keepdims=True)\n",
    "        focal_weights = tf.pow(1 - probs, gamma)\n",
    "        \n",
    "        # Apply class weights\n",
    "        per_class_weights = class_weights * y_true_oh\n",
    "        \n",
    "        # Compute focal loss\n",
    "        focal_loss_per_pixel = tf.reduce_sum(per_class_weights * cross_entropy * focal_weights, axis=-1)\n",
    "        \n",
    "        return tf.reduce_mean(focal_loss_per_pixel)\n",
    "    return static_focal_loss\n",
    "\n",
    "mean_io_u = tf.keras.metrics.MeanIoU(\n",
    "    num_classes=num_classes,\n",
    "    ignore_class=0,\n",
    "    sparse_y_pred=False,\n",
    "    name='mean_io_u'\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=focal_loss(class_weights=class_weights),\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy', mean_io_u]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when()\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Dataset split\n",
    "training, validation = fold.split_masked_set(data, shuffle=True, size=0.1, starting=np.random.random())\n",
    "\n",
    "# Train model epoch\n",
    "history = model.fit(\n",
    "    training['images'], training['labels'],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(validation['images'], validation['labels']),\n",
    "    callbacks=[reduce_lr, early_stopping, model_checkpoint, QuietCallback()],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print()\n",
    "when()\n",
    "\n",
    "# Save model\n",
    "model_filename = f\"{model_name}.keras\"\n",
    "model.save(model_filename)\n",
    "print(f\"Model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when()\n",
    "\n",
    "print(list(history.history.keys()))\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Mean IoU plot\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(history.history['mean_io_u'], label='Train Mean IoU')\n",
    "plt.plot(history.history['val_mean_io_u'], label='Validation Mean IoU')\n",
    "plt.title('Mean IoU Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.legend()\n",
    "\n",
    "# Learning rate plot\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(history.history['learning_rate'], label='Learning Rate')\n",
    "plt.title('Learning Rate Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNp6pUZuddqC"
   },
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s18kX1uDconq"
   },
   "outputs": [],
   "source": [
    "when()\n",
    "\n",
    "predictions = model.predict(test)\n",
    "predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "def y_to_df(y) -> pd.DataFrame:\n",
    "    \"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n",
    "    n_samples = len(y)\n",
    "    y_flat = y.reshape(n_samples, -1)\n",
    "    df = pd.DataFrame(y_flat)\n",
    "    df[\"id\"] = np.arange(n_samples)\n",
    "    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n",
    "    return df[cols]\n",
    "\n",
    "submission_filename = f\"{model_name}.csv\"\n",
    "submission_dataframe = y_to_df(predictions)\n",
    "submission_dataframe.to_csv(submission_filename, index=False)\n",
    "print(f\"Submission saved as {submission_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when()\n",
    "\n",
    "def random_plot():\n",
    "    i = random.randint(0, len(predictions)-1)\n",
    "    j = random.randint(0, len(predictions)-1)\n",
    "    k = random.randint(0, len(predictions)-1)\n",
    "    plot.plot_masked_image(        \n",
    "        (test.squeeze()[i], predictions[i]),\n",
    "        (test.squeeze()[j], predictions[j]),\n",
    "        (test.squeeze()[k], predictions[k]),\n",
    "        mask_alpha=0.1)\n",
    "    plot.plot_masked_image(        \n",
    "        (test.squeeze()[i], predictions[i]),\n",
    "        (test.squeeze()[j], predictions[j]),\n",
    "        (test.squeeze()[k], predictions[k]),\n",
    "        mask_alpha=0)\n",
    "    plot.plot_masked_image(        \n",
    "        (test.squeeze()[i], predictions[i]),\n",
    "        (test.squeeze()[j], predictions[j]),\n",
    "        (test.squeeze()[k], predictions[k]),\n",
    "        mask_alpha=1)\n",
    "\n",
    "print([f\"{w:.4f}\" for w in class_weights])\n",
    "counts = np.bincount(predictions.flatten())\n",
    "rates = counts / np.sum(counts)\n",
    "print([f\"{r:.4f}\" for r in rates])\n",
    "for _ in range(4):\n",
    "    random_plot()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6237643,
     "sourceId": 10110873,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6297027,
     "sourceId": 10191717,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6298222,
     "sourceId": 10193325,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 211071978,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 211079960,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
