{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6No-KjqbPAb"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnk39HgI2j4A"
   },
   "source": [
    "##### Import content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9N0vC0VyOk5D",
    "outputId": "859f1e86-85bb-4815-8a8f-c2cdb451ee91"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd '/content/drive/My Drive/NNN_ANNDL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc2Da0c1fbVU"
   },
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ro1qHUu5dxaX"
   },
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install keras_cv\n",
    "\n",
    "# Standard libraries\n",
    "import math\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import gc\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import mixed_precision\n",
    "from keras.layers import GroupNormalization\n",
    "\n",
    "\n",
    "# KerasCV library\n",
    "import keras_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMG9X1ETXOAi"
   },
   "source": [
    "# Augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJKXWpS-caps"
   },
   "outputs": [],
   "source": [
    "# Default function to augment data, compatible with .map\n",
    "def augfn(fn, image):\n",
    "    image = fn(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VndjAaikX3HH"
   },
   "outputs": [],
   "source": [
    "# SCALE (True scale - mean is not 0)\n",
    "scale_augmentations = [\n",
    "    keras_cv.layers.RandomZoom(\n",
    "        height_factor=[0, 0.5],\n",
    "        width_factor=[0, 0.5],\n",
    "        fill_mode=\"nearest\",\n",
    "        interpolation=\"bilinear\"\n",
    "    ),\n",
    "    keras_cv.layers.RandomZoom(\n",
    "        height_factor=[-0.5, 0],\n",
    "        width_factor=[-0.5, 0],\n",
    "        fill_mode=\"nearest\",\n",
    "        interpolation=\"bilinear\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create a RandomAugmentationPipeline with scale augmentation\n",
    "scale_augment = keras_cv.layers.RandomAugmentationPipeline(\n",
    "    layers=scale_augmentations,\n",
    "    augmentations_per_image=1,\n",
    "    rate=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXV9y98EiU9m"
   },
   "outputs": [],
   "source": [
    "# Define the AugMix, GridMask, and RandAugment augmentation layers\n",
    "augmix = [\n",
    "    keras_cv.layers.AugMix(\n",
    "        value_range=[0, 1],\n",
    "        severity=0.65,\n",
    "        num_chains=3,\n",
    "        chain_depth=[1, 3]\n",
    "    ),\n",
    "    keras_cv.layers.GridMask(\n",
    "        ratio_factor=(0, 0.3),\n",
    "        rotation_factor=0.6,\n",
    "        fill_mode=\"constant\",\n",
    "        fill_value=0\n",
    "    ),\n",
    "    keras_cv.layers.RandAugment(\n",
    "        value_range=[0, 1],\n",
    "        augmentations_per_image=3,\n",
    "        magnitude=0.6,\n",
    "        magnitude_stddev=0.3\n",
    "    )\n",
    "]\n",
    "\n",
    "# Combine the augmentations in a RandomAugmentationPipeline\n",
    "augmix_augment = keras_cv.layers.RandomAugmentationPipeline(\n",
    "    layers=augmix,\n",
    "    augmentations_per_image=2,\n",
    "    rate=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJB560EKWjXP"
   },
   "outputs": [],
   "source": [
    "# Validation augmentations (more severe than training)\n",
    "valaug = [\n",
    "    keras_cv.layers.AugMix(\n",
    "        value_range=[0, 1],\n",
    "        severity=0.8,\n",
    "        num_chains=5,\n",
    "        chain_depth=[1, 5]\n",
    "    ),\n",
    "    keras_cv.layers.GridMask(\n",
    "        ratio_factor=(0, 0.3),\n",
    "        rotation_factor=0.6,\n",
    "        fill_mode=\"constant\",\n",
    "        fill_value=0\n",
    "    ),\n",
    "    keras_cv.layers.AutoContrast(\n",
    "        value_range=[0, 1]\n",
    "    ),\n",
    "    keras_cv.layers.RandAugment(\n",
    "        value_range=[0, 1],\n",
    "        augmentations_per_image=3,\n",
    "        magnitude=0.7,\n",
    "        magnitude_stddev=0.15\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create a RandomAugmentationPipeline for validation augmentations\n",
    "valaug_augment = keras_cv.layers.RandomAugmentationPipeline(\n",
    "    layers=valaug,\n",
    "    augmentations_per_image=2,\n",
    "    rate=0.8\n",
    ")\n",
    "\n",
    "# Validation scaling augmentations\n",
    "valscale = [\n",
    "    keras_cv.layers.RandomZoom(\n",
    "        height_factor=[0, 0.5],\n",
    "        width_factor=[0, 0.5],\n",
    "        fill_mode=\"nearest\",\n",
    "        interpolation=\"bilinear\"\n",
    "    ),\n",
    "    keras_cv.layers.RandomZoom(\n",
    "        height_factor=[-0.5, 0],\n",
    "        width_factor=[-0.5, 0],\n",
    "        fill_mode=\"nearest\",\n",
    "        interpolation=\"bilinear\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create a RandomAugmentationPipeline for validation scale augmentations\n",
    "valscale_augment = keras_cv.layers.RandomAugmentationPipeline(\n",
    "    layers=valscale,\n",
    "    augmentations_per_image=1,\n",
    "    rate=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc-c_LWaGJYi"
   },
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDozi6kbui6l",
    "outputId": "c758bbc4-007a-41fc-eb32-f068f4734184"
   },
   "outputs": [],
   "source": [
    "# Load outlier filtered dataset\n",
    "data = np.load(\"Datasets/dataset_filtered.npz\")\n",
    "images = data['images']\n",
    "labels = np.squeeze(data['labels'])\n",
    "\n",
    "# Analyze label distribution\n",
    "label_counts = Counter(labels.tolist())\n",
    "max_count = max(label_counts.values())\n",
    "\n",
    "# Oversample the dataset\n",
    "def oversample_data(images, labels):\n",
    "    oversampled_images = []\n",
    "    oversampled_labels = []\n",
    "\n",
    "    for label in range(len(label_counts)):\n",
    "        # Filter samples of the current label\n",
    "        label_indices = np.where(labels == label)[0]\n",
    "        current_images = images[label_indices]\n",
    "\n",
    "        # Oversample to match max_count\n",
    "        num_to_add = max_count - len(current_images)\n",
    "        if num_to_add > 0:\n",
    "            extra_images = current_images[np.random.choice(len(current_images), num_to_add)]\n",
    "            oversampled_images.extend(extra_images)\n",
    "            oversampled_labels.extend([label] * num_to_add)\n",
    "\n",
    "        # Add the existing images and labels\n",
    "        oversampled_images.extend(current_images)\n",
    "        oversampled_labels.extend([label] * len(current_images))\n",
    "\n",
    "    return np.array(oversampled_images), np.array(oversampled_labels)\n",
    "\n",
    "# Balance the dataset\n",
    "balanced_images, balanced_labels = oversample_data(images, labels)\n",
    "\n",
    "# Train-validation split (80%-20%)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    balanced_images, balanced_labels, test_size=0.2, stratify=balanced_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Free up memory by deleting unnecessary variables\n",
    "del images, labels, balanced_images, balanced_labels, label_counts, max_count\n",
    "\n",
    "# Create preprocess, augment and postprocess functions\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)  # Normalize to [0, 1]\n",
    "    return image, label\n",
    "\n",
    "def postprocess_image(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.uint8)  # Normalize to [0, 255]\n",
    "    return image, label\n",
    "\n",
    "def base_augment_image(image, label):\n",
    "    image = augfn(scale_augment, image)\n",
    "    return image, label\n",
    "\n",
    "def mix_augment_image(image, label):\n",
    "    image = augfn(augmix_augment, image)\n",
    "    return image, label\n",
    "\n",
    "def val_augment_image(image, label):\n",
    "    image = augfn(valscale_augment, image)\n",
    "    image = augfn(valaug_augment, image)\n",
    "    return image, label\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = (\n",
    "    train_dataset\n",
    "    .shuffle(len(train_images))  # Shuffle the training set\n",
    "    .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)  # Normalize images\n",
    "    .map(base_augment_image, num_parallel_calls=tf.data.AUTOTUNE)  # Apply augmentations\n",
    "    .map(mix_augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .map(postprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(1024)  # Batch size\n",
    "    .prefetch(tf.data.AUTOTUNE)  # Prefetch for performance\n",
    ")\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = (\n",
    "    val_dataset\n",
    "    .map(preprocess_image)  # preprocess validation images\n",
    "    .map(val_augment_image, num_parallel_calls=tf.data.AUTOTUNE) # Val set augmentations\n",
    "    .map(postprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(512)  # Batch size\n",
    "    .prefetch(tf.data.AUTOTUNE)  # Prefetch for performance\n",
    ")\n",
    "\n",
    "# Free up memory by deleting train and validation arrays\n",
    "del train_images, train_labels, val_images, val_labels\n",
    "\n",
    "# Test if datasets are correctly initialized\n",
    "image_batch, label_batch = next(iter(train_dataset.take(1)))\n",
    "\n",
    "print(f\"Image batch shape: {image_batch.shape}\")\n",
    "print(f\"Label batch shape: {label_batch.shape}\")\n",
    "\n",
    "print(f\"Image max:{np.max(image_batch)}\")\n",
    "print(f\"Image type:{image_batch.dtype}\")\n",
    "print(f\"Label type:{label_batch.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 967
    },
    "id": "CjgETIi-teBS",
    "outputId": "c5410a08-fc67-4f43-cbcd-e18cedd3a8b2"
   },
   "outputs": [],
   "source": [
    "def plot_9_samples(dataset, label_names=None, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Plots 9 images from the dataset with their respective labels,\n",
    "    resizing the images to the specified target size.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    resizing_layer = tf.keras.layers.Resizing(*target_size)  # Create resizing layer\n",
    "\n",
    "    for i, sample in enumerate(dataset.take(9)):  # Take 9 samples\n",
    "        # Extract image and label\n",
    "        image = sample[0][0, ...]  # Take the first image in the batch\n",
    "        label = sample[1][0, ...]  # Take the first label in the batch\n",
    "\n",
    "        # Resize the image\n",
    "        resized_image = resizing_layer(image)\n",
    "\n",
    "        # Convert to numpy for plotting\n",
    "        resized_image_np = np.array(resized_image)\n",
    "        print(np.max(resized_image_np))\n",
    "        if label_names is not None:\n",
    "            # Map label index to label name\n",
    "            label_text = label_names[int(label)]\n",
    "        else:\n",
    "            # Fallback to just displaying the label index if label names not provided\n",
    "            label_text = str(label)\n",
    "\n",
    "        # Plot the image with its label\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.squeeze(resized_image_np))  # Squeeze in case images are grayscale\n",
    "        plt.title(f\"Label: {label_text}\", fontsize=10)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Label names for visualization\n",
    "label_names = {\n",
    "    0: \"Basophil\",\n",
    "    1: \"Eosinophil\",\n",
    "    2: \"Erythroblast\",\n",
    "    3: \"Granulocyte\",\n",
    "    4: \"Lymphocyte\",\n",
    "    5: \"Monocyte\",\n",
    "    6: \"Neutrophil\",\n",
    "    7: \"Platelet\"\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "plot_9_samples(train_dataset, label_names=label_names, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMzFOQ4dA26Z"
   },
   "source": [
    "# Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wGCW25hEElrJ",
    "outputId": "cb73418a-4bda-477e-b9b7-ad963dd2fa1a"
   },
   "outputs": [],
   "source": [
    "# Set precision policy\n",
    "mixed_precision.set_global_policy('bfloat16')\n",
    "\n",
    "#Define build model function\n",
    "def build_model(model_key, input_shape=(96, 96, 3), num_classes=8):\n",
    "    model_name, layer_name = model_dict[model_key]\n",
    "    base_model_class = getattr(tfk.applications, model_name)\n",
    "    base_model = base_model_class(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tfk.layers.Input(shape=(96, 96, 3))\n",
    "    x = base_model(inputs)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = GroupNormalization(groups=32, axis=-1)(x)\n",
    "    x = tfk.layers.Dropout(0.2)(x)\n",
    "    x = tfk.layers.Dense(512, activation='gelu', kernel_regularizer=tfk.regularizers.l2(0.001))(x)\n",
    "    x = GroupNormalization(groups=32, axis=-1)(x)\n",
    "    x = tfk.layers.Dropout(0.2)(x)\n",
    "    x = tfk.layers.Dense(128, activation='gelu', kernel_regularizer=tfk.regularizers.l2(0.001))(x)\n",
    "    x = GroupNormalization(groups=32, axis=-1)(x)\n",
    "    outputs = tfk.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = tfk.Model(inputs=inputs, outputs=outputs, trainable=True)\n",
    "\n",
    "    # Compile to check integrity\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Lion(\n",
    "            learning_rate=1e-3,\n",
    "            weight_decay=1e-4\n",
    "        ),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model, base_model\n",
    "\n",
    "# Define a model dict where a key is defined to select a tuple containing ('ModelNameFromKerasLibrary', 'base_model_layer_name')\n",
    "model_dict = {\n",
    "    'MV3L': ('MobileNetV3Large', 'MobileNetV3Large'),\n",
    "    'EfV2M': ('EfficientNetV2M', 'efficientnetv2-m'),\n",
    "    'CNT': ('ConvNeXtTiny', 'convnext_tiny'),\n",
    "    'CNS': ('ConvNeXtSmall', 'convnext_small'),\n",
    "    'CNB': ('ConvNeXtBase', 'convnext_base'),\n",
    "    'MV2': ('MobileNetV2', 'MobileNetV2')\n",
    "}\n",
    "\n",
    "# Use the model key to swiftly change used base model\n",
    "model_key = 'CNT'\n",
    "model, base_model = build_model(model_key)\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z67GYuCdFLpz"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gajVolcOIY18"
   },
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "P9IPmVuOE-d7",
    "outputId": "458adb65-96f4-4442-e57e-0e3b005ed903"
   },
   "outputs": [],
   "source": [
    "# Create a timestamp string\n",
    "timestamp = datetime.now().strftime(\"%d_%H-%M-%S\")\n",
    "\n",
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f\"Submissions/Lolli/{model_key}_{timestamp}.keras\",\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-4,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Recompile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Lion(\n",
    "        learning_rate=1e-3,\n",
    "        weight_decay=0.1\n",
    "    ),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nStarting top layers training phase...\")\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping, checkpoint, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Clean up\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8gYserOIqba"
   },
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2b943de_7az",
    "outputId": "e026df85-888e-4910-8f39-2f6b39428732"
   },
   "outputs": [],
   "source": [
    "# Define the function to keep the pretrained normalization layers frozen\n",
    "def set_bn_to_inference_mode(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "# Gradual unfreezing layers function\n",
    "def unfreeze_and_train(model, base_model, train_generator, val_generator,\n",
    "                       initial_lr=1e-5, unfreeze_percentage=0.2,\n",
    "                       epochs_per_stage=5, min_lr=1e-7):\n",
    "    \"\"\"\n",
    "    Progressively unfreeze and train layers of the base model.\n",
    "\n",
    "    Args:\n",
    "        model: The full model including base model and top layers\n",
    "        base_model: The base model portion (e.g., MobileNetV3Large)\n",
    "        train_generator: Training data generator\n",
    "        val_generator: Validation data generator\n",
    "        initial_lr: Starting learning rate\n",
    "        unfreeze_percentage: Percentage of layers to unfreeze in each stage\n",
    "        epochs_per_stage: Number of epochs to train in each stage\n",
    "        min_lr: Minimum learning rate before stopping\n",
    "    \"\"\"\n",
    "    base_model_layers = base_model.layers\n",
    "    total_base_model_layers = len(base_model_layers)\n",
    "    layers_to_unfreeze = math.ceil(total_base_model_layers * unfreeze_percentage)\n",
    "    current_unfrozen_layers = 0\n",
    "    current_lr = initial_lr\n",
    "    best_val_accuracy = 0\n",
    "    stages_without_improvement = 0\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%d_%H-%M-%S\")\n",
    "    # Create callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f\"Submissions/Lolli/{model_key}_{timestamp}_stage_{current_unfrozen_layers}.keras\",\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=min_lr,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Dictionary to store training history\n",
    "    training_history = {}\n",
    "\n",
    "    while current_unfrozen_layers < total_base_model_layers:\n",
    "        stage = current_unfrozen_layers // layers_to_unfreeze + 1\n",
    "\n",
    "        # Calculate layers to unfreeze for this stage\n",
    "        end_layer = min(current_unfrozen_layers + layers_to_unfreeze, total_base_model_layers)\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Starting Stage {stage}, unfreezin {layers_to_unfreeze} layers\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Unfreeze layers for this stage\n",
    "        for i, layer in enumerate(base_model_layers):\n",
    "            if i < end_layer:\n",
    "                layer.trainable = True\n",
    "            else:\n",
    "                layer.trainable = False\n",
    "\n",
    "        set_bn_to_inference_mode(model)\n",
    "\n",
    "        # Compile model with current learning rate\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Lion(\n",
    "                learning_rate=current_lr,\n",
    "                weight_decay=0.1\n",
    "            ),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # Train for this stage\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=val_generator,\n",
    "            epochs=epochs_per_stage,\n",
    "            callbacks=[checkpoint, early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Store history for this stage\n",
    "        training_history[f'stage_{stage}'] = {\n",
    "            'accuracy': history.history['accuracy'],\n",
    "            'val_accuracy': history.history['val_accuracy'],\n",
    "            'loss': history.history['loss'],\n",
    "            'val_loss': history.history['val_loss']\n",
    "        }\n",
    "\n",
    "        # Check if we should continue training\n",
    "        current_val_accuracy = max(history.history['val_accuracy'])\n",
    "        if current_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = current_val_accuracy\n",
    "            stages_without_improvement = 0\n",
    "        else:\n",
    "            stages_without_improvement += 1\n",
    "\n",
    "        # Early stopping across stages\n",
    "        if stages_without_improvement >= 2:\n",
    "            print(\"\\nStopping early due to no improvement across stages\")\n",
    "            break\n",
    "\n",
    "        # Update learning rate and unfrozen layers count\n",
    "        current_lr = max(current_lr * 0.5, min_lr)\n",
    "        if current_lr <= min_lr:\n",
    "            print(\"\\nReached minimum learning rate, stopping training\")\n",
    "            break\n",
    "\n",
    "        current_unfrozen_layers = end_layer\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    return training_history\n",
    "\n",
    "# Define plot training history function\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for stage in history:\n",
    "        plt.plot(history[stage]['accuracy'], label=f'{stage}_train')\n",
    "        plt.plot(history[stage]['val_accuracy'], label=f'{stage}_val')\n",
    "    plt.title('Model Accuracy Across Stages')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for stage in history:\n",
    "        plt.plot(history[stage]['loss'], label=f'{stage}_train')\n",
    "        plt.plot(history[stage]['val_loss'], label=f'{stage}_val')\n",
    "    plt.title('Model Loss Across Stages')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Start the training\n",
    "training_history = unfreeze_and_train(\n",
    "    model=model,\n",
    "    base_model=base_model,\n",
    "    train_generator=train_dataset,\n",
    "    val_generator=val_dataset,\n",
    "    initial_lr=1e-3,\n",
    "    unfreeze_percentage=1,\n",
    "    epochs_per_stage=30,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "# Plot History\n",
    "plot_training_history(training_history)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "B6No-KjqbPAb",
    "fnk39HgI2j4A",
    "Xc2Da0c1fbVU",
    "EMG9X1ETXOAi",
    "Bc-c_LWaGJYi",
    "wMzFOQ4dA26Z",
    "z67GYuCdFLpz",
    "gajVolcOIY18"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
